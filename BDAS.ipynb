{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Spark to read csv file\n",
    "SuicideStats = spark.read.csv(\"data/who_suicide_statistics.csv\",inferSchema=True,header=True)\n",
    "HDIStats = spark.read.csv(\"data/Human Development Index.csv\",inferSchema=True,header=True)\n",
    "WDIStats = spark.read.csv(\"data/World_Development_Indicators.csv\",inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43776, 6)\n"
     ]
    }
   ],
   "source": [
    "#Data Exploration\n",
    "\n",
    "#Explore the volumn of the dataset WHO Suicide Statistics\n",
    "print((SuicideStats.count(), len(SuicideStats.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189, 31)\n"
     ]
    }
   ],
   "source": [
    "#Explore the volumn of the dataset Human Development Index\n",
    "print((HDIStats.count(), len(HDIStats.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6944, 6)\n"
     ]
    }
   ],
   "source": [
    "#Explore the volumn of the dataset World Development Indicators\n",
    "print((WDIStats.count(), len(WDIStats.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['country', 'year', 'sex', 'age', 'suicides_no', 'population']\n",
      "+-------+----+------+-----------+-----------+----------+\n",
      "|country|year|   sex|        age|suicides_no|population|\n",
      "+-------+----+------+-----------+-----------+----------+\n",
      "|Albania|1985|female|15-24 years|       null|    277900|\n",
      "|Albania|1985|female|25-34 years|       null|    246800|\n",
      "|Albania|1985|female|35-54 years|       null|    267500|\n",
      "|Albania|1985|female| 5-14 years|       null|    298300|\n",
      "|Albania|1985|female|55-74 years|       null|    138700|\n",
      "|Albania|1985|female|  75+ years|       null|     34200|\n",
      "+-------+----+------+-----------+-----------+----------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Explore the data quality of the dataset WHO Suicide Statistics\n",
    "print(SuicideStats.columns)\n",
    "SuicideStats.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- suicides_no: integer (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Explore the data type of the dataset WHO Suicide Statistics\n",
    "SuicideStats.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   sex|\n",
      "+------+\n",
      "|female|\n",
      "|  male|\n",
      "+------+\n",
      "\n",
      "+-----------+\n",
      "|        age|\n",
      "+-----------+\n",
      "|55-74 years|\n",
      "|25-34 years|\n",
      "| 5-14 years|\n",
      "|  75+ years|\n",
      "|15-24 years|\n",
      "|35-54 years|\n",
      "+-----------+\n",
      "\n",
      "141\n",
      "+--------------------+\n",
      "|             country|\n",
      "+--------------------+\n",
      "|            Anguilla|\n",
      "|            Paraguay|\n",
      "|          Cabo Verde|\n",
      "|              Sweden|\n",
      "|            Kiribati|\n",
      "|   Republic of Korea|\n",
      "|              Guyana|\n",
      "|         Philippines|\n",
      "|Occupied Palestin...|\n",
      "|            Malaysia|\n",
      "|           Singapore|\n",
      "|                Fiji|\n",
      "|              Turkey|\n",
      "|      TFYR Macedonia|\n",
      "|                Iraq|\n",
      "|             Germany|\n",
      "|              Jordan|\n",
      "|            Maldives|\n",
      "|              France|\n",
      "|Turks and Caicos ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SuicideStats.select(\"sex\").distinct().show()\n",
    "SuicideStats.select(\"age\").distinct().show()\n",
    "print(SuicideStats.select(\"country\").distinct().count())\n",
    "SuicideStats.select(\"country\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------------+------+-----------+-----------------+------------------+\n",
      "|summary| country|              year|   sex|        age|      suicides_no|        population|\n",
      "+-------+--------+------------------+------+-----------+-----------------+------------------+\n",
      "|  count|   43776|             43776| 43776|      43776|            41520|             38316|\n",
      "|   mean|    null|1998.5024671052631|  null|       null|193.3153901734104|1664091.1353742562|\n",
      "| stddev|    null|10.338711176746282|  null|       null|800.5899259349637|3647231.2274873867|\n",
      "|    min| Albania|              1979|female|15-24 years|                0|               259|\n",
      "|    max|Zimbabwe|              2016|  male|  75+ years|            22338|          43805214|\n",
      "+-------+--------+------------------+------+-----------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Explore the general statistics of the dataset WHO Suicide Statistics\n",
    "SuicideStats.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HDI Rank', 'Country', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']\n",
      "+--------+-----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|HDI Rank|          Country| 1990| 1991| 1992| 1993| 1994| 1995| 1996| 1997| 1998| 1999| 2000| 2001| 2002| 2003| 2004| 2005| 2006| 2007| 2008| 2009| 2010| 2011| 2012| 2013| 2014| 2015| 2016| 2017| 2018|\n",
      "+--------+-----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|     170|      Afghanistan|0.298|0.304|0.312|0.308|0.303|0.327|0.331|0.335|0.339|0.343|0.345|0.347|0.378|0.387|  0.4| 0.41|0.419|0.431|0.436|0.447|0.464|0.465|0.479|0.485|0.488| 0.49|0.491|0.493|0.496|\n",
      "|      69|          Albania|0.644|0.625|0.608|0.611|0.617|0.629|0.639|0.639|0.649| 0.66|0.667|0.673| 0.68|0.687|0.692|0.702|0.709|0.718|0.724|0.729| 0.74|0.759|0.771|0.781|0.787|0.788|0.788|0.789|0.791|\n",
      "|      82|          Algeria|0.578|0.582|0.589|0.593|0.597|0.602| 0.61|0.619|0.629|0.638|0.646|0.655|0.666|0.676|0.685|0.694|0.699|0.708|0.711| 0.72| 0.73|0.738|0.737|0.746|0.749|0.751|0.755|0.758|0.759|\n",
      "|      36|          Andorra|   ..|   ..|   ..|   ..|   ..|   ..|   ..|   ..|   ..|   ..|0.759|0.767| 0.78| 0.82|0.826|0.819|0.829|0.829|0.831| 0.83|0.828|0.827|0.849|0.846|0.853| 0.85|0.854|0.852|0.857|\n",
      "|     149|           Angola|   ..|   ..|   ..|   ..|   ..|   ..|   ..|   ..|   ..|0.384|0.394|0.404|0.419|0.428| 0.44|0.453|0.466|0.482|0.494|0.508| 0.51|0.525|0.537|0.547|0.557|0.565| 0.57|0.576|0.574|\n",
      "|      74|AntiguaandBarbuda|   ..|   ..|   ..|   ..|   ..|   ..|   ..|   ..|   ..|   ..|   ..|   ..|   ..|   ..|   ..|0.773|0.779|0.784|0.783|0.776|0.772|0.764|0.767|0.767|0.767| 0.77|0.772|0.774|0.776|\n",
      "+--------+-----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Explore the data quality of the dataset Human Development Index\n",
    "print(HDIStats.columns)\n",
    "HDIStats.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HDI Rank: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- 1990: string (nullable = true)\n",
      " |-- 1991: string (nullable = true)\n",
      " |-- 1992: string (nullable = true)\n",
      " |-- 1993: string (nullable = true)\n",
      " |-- 1994: string (nullable = true)\n",
      " |-- 1995: string (nullable = true)\n",
      " |-- 1996: string (nullable = true)\n",
      " |-- 1997: string (nullable = true)\n",
      " |-- 1998: string (nullable = true)\n",
      " |-- 1999: string (nullable = true)\n",
      " |-- 2000: string (nullable = true)\n",
      " |-- 2001: string (nullable = true)\n",
      " |-- 2002: string (nullable = true)\n",
      " |-- 2003: string (nullable = true)\n",
      " |-- 2004: string (nullable = true)\n",
      " |-- 2005: string (nullable = true)\n",
      " |-- 2006: string (nullable = true)\n",
      " |-- 2007: string (nullable = true)\n",
      " |-- 2008: string (nullable = true)\n",
      " |-- 2009: string (nullable = true)\n",
      " |-- 2010: string (nullable = true)\n",
      " |-- 2011: string (nullable = true)\n",
      " |-- 2012: string (nullable = true)\n",
      " |-- 2013: string (nullable = true)\n",
      " |-- 2014: string (nullable = true)\n",
      " |-- 2015: string (nullable = true)\n",
      " |-- 2016: string (nullable = true)\n",
      " |-- 2017: double (nullable = true)\n",
      " |-- 2018: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Explore the data type of the dataset Human Development Index\n",
    "HDIStats.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------+-------------------+------------------+-------------------+-------------------+-------------------+------------------+-------------------+------------------+------------------+-----------------+-------------------+-------------------+-------------------+------------------+-------------------+-------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+-------------------+------------------+-------------------+------------------+-------------------+-------------------+\n",
      "|summary|         HDI Rank|    Country|               1990|              1991|               1992|               1993|               1994|              1995|               1996|              1997|              1998|             1999|               2000|               2001|               2002|              2003|               2004|               2005|              2006|               2007|               2008|               2009|               2010|               2011|              2012|               2013|              2014|               2015|              2016|               2017|               2018|\n",
      "+-------+-----------------+-----------+-------------------+------------------+-------------------+-------------------+-------------------+------------------+-------------------+------------------+------------------+-----------------+-------------------+-------------------+-------------------+------------------+-------------------+-------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+-------------------+------------------+-------------------+------------------+-------------------+-------------------+\n",
      "|  count|              189|        189|                189|               189|                189|                189|                189|               189|                189|               189|               189|              189|                189|                189|                189|               189|                189|                189|               189|                189|                189|                189|                189|                189|               189|                189|               189|                189|               189|                189|                189|\n",
      "|   mean|94.77248677248677|       null| 0.5949513888888888|0.5971597222222224| 0.5969790209790211| 0.6028611111111108| 0.6065277777777774|0.6103445945945944|  0.615858108108108|0.6191700680272109|0.6262432432432437|0.626814569536424|   0.62732183908046| 0.6329540229885059| 0.6388800000000002| 0.643994350282486| 0.6505307262569835| 0.6540537634408602|0.6611290322580645| 0.6681075268817206| 0.6741612903225808|              0.678|  0.681712765957447| 0.6874202127659573|0.6922659574468086|  0.697127659574468|0.7013989361702128| 0.7053510638297874| 0.708489361702128| 0.7112222222222223| 0.7134708994708996|\n",
      "| stddev|54.74045223905498|       null|0.16559426064600388|0.1659663654186352|0.16557515924678484|0.16770526973848346|0.17017526729998575|0.1689925616379318|0.16869249123917165|0.1681109945696617|0.1693128015950585| 0.17098102834227|0.16947293297641114|0.16910990427413294|0.16791377918854467|0.1680289645342278|0.16595157309405328|0.16487937379999615| 0.163487910177461|0.16223995991027756|0.16067884977099967|0.15777250128638434|0.15668153758878414|0.15506161968527132|0.1538444367098304|0.15355305195799956|0.1526419928026906|0.15229314593351687|0.1521042243771921|0.15121385917194813|0.15079461673378391|\n",
      "|    min|                1|Afghanistan|                 ..|                ..|                 ..|                 ..|                 ..|                ..|                 ..|                ..|                ..|               ..|                 ..|                 ..|                 ..|                ..|                 ..|                 ..|                ..|                 ..|                 ..|                 ..|                 ..|                 ..|                ..|                 ..|                ..|                 ..|                ..|              0.373|              0.377|\n",
      "|    max|              189|   Zimbabwe|              0.866|             0.867|              0.868|              0.872|              0.885|             0.883|              0.888|             0.894|             0.906|            0.911|              0.917|              0.916|              0.918|             0.924|              0.934|              0.932|             0.936|              0.938|              0.938|              0.938|              0.942|              0.943|             0.942|              0.946|             0.945|              0.948|             0.951|              0.953|              0.954|\n",
      "+-------+-----------------+-----------+-------------------+------------------+-------------------+-------------------+-------------------+------------------+-------------------+------------------+------------------+-----------------+-------------------+-------------------+-------------------+------------------+-------------------+-------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+-------------------+------------------+-------------------+------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Explore the general statistics of the dataset Human Development Index\n",
    "HDIStats.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Country Name', 'Year', 'Employment to population ratio', 'GDP', 'GDP per capita', 'Gini index']\n",
      "+------------+----+------------------------------+----+--------------+----------+\n",
      "|Country Name|Year|Employment to population ratio| GDP|GDP per capita|Gini index|\n",
      "+------------+----+------------------------------+----+--------------+----------+\n",
      "| Afghanistan|1985|                          null|null|          null|      null|\n",
      "| Afghanistan|1986|                          null|null|          null|      null|\n",
      "| Afghanistan|1987|                          null|null|          null|      null|\n",
      "| Afghanistan|1988|                          null|null|          null|      null|\n",
      "| Afghanistan|1989|                          null|null|          null|      null|\n",
      "| Afghanistan|1990|                          null|null|          null|      null|\n",
      "+------------+----+------------------------------+----+--------------+----------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Explore the data quality of the dataset World Development Indicators\n",
    "print(WDIStats.columns)\n",
    "WDIStats.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country Name: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Employment to population ratio: double (nullable = true)\n",
      " |-- GDP: double (nullable = true)\n",
      " |-- GDP per capita: double (nullable = true)\n",
      " |-- Gini index: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Explore the data type of the dataset World Development Indicators\n",
    "WDIStats.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----------------+------------------------------+--------------------+------------------+-----------------+\n",
      "|summary|Country Name|             Year|Employment to population ratio|                 GDP|    GDP per capita|       Gini index|\n",
      "+-------+------------+-----------------+------------------------------+--------------------+------------------+-----------------+\n",
      "|  count|        6944|             6944|                          4862|                5979|              5976|             1550|\n",
      "|   mean|        null|           2000.5|              57.6225362242226|2.745890309848958...| 13124.04119241654|38.62148387096775|\n",
      "| stddev|        null|9.233757553344075|            11.968146617150545|1.111967617317697E12|20099.670239668914|9.432100956926329|\n",
      "|    min| Afghanistan|             1985|                   26.32999992|       2.144196696E7|       164.1918663|             20.2|\n",
      "|    max|    Zimbabwe|             2016|                    90.4469986|          1.69723E13|       194368.4055|             65.8|\n",
      "+-------+------------+-----------------+------------------------------+--------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Explore the general statistics of the dataset World Development Indicators\n",
    "WDIStats.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---+---+-----------+----------+\n",
      "|country|year|sex|age|suicides_no|population|\n",
      "+-------+----+---+---+-----------+----------+\n",
      "|      0|   0|  0|  0|       2256|      5460|\n",
      "+-------+----+---+---+-----------+----------+\n",
      "\n",
      "+-------+-------+-----+-----+-----+-----------+----------+\n",
      "|summary|country| year|  sex|  age|suicides_no|population|\n",
      "+-------+-------+-----+-----+-----+-----------+----------+\n",
      "|  count|  43776|43776|43776|43776|      41520|     38316|\n",
      "+-------+-------+-----+-----+-----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Explore missing values in the dataset WHO Suicide Statistics\n",
    "from pyspark.sql.functions import when, count, col\n",
    "\n",
    "SuicideStats.select([count(when(col(c).isNull(), c)).alias(c) for c in SuicideStats.columns]).show()\n",
    "SuicideStats.describe().filter(col(\"summary\")  == \"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|HDI Rank|Country|1990|1991|1992|1993|1994|1995|1996|1997|1998|1999|2000|2001|2002|2003|2004|2005|2006|2007|2008|2009|2010|2011|2012|2013|2014|2015|2016|2017|2018|\n",
      "+--------+-------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|       0|      0|  45|  45|  46|  45|  45|  41|  41|  42|  41|  38|  15|  15|  14|  12|  10|   3|   3|   3|   3|   3|   1|   1|   1|   1|   1|   1|   1|   0|   0|\n",
      "+--------+-------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "\n",
      "+-------+--------+-------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|summary|HDI Rank|Country|1990|1991|1992|1993|1994|1995|1996|1997|1998|1999|2000|2001|2002|2003|2004|2005|2006|2007|2008|2009|2010|2011|2012|2013|2014|2015|2016|2017|2018|\n",
      "+-------+--------+-------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|  count|     189|    189| 144| 144| 143| 144| 144| 148| 148| 147| 148| 151| 174| 174| 175| 177| 179| 186| 186| 186| 186| 186| 188| 188| 188| 188| 188| 188| 188| 189| 189|\n",
      "+-------+--------+-------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Re-import dataset, taking .. as null value\n",
    "HDIStats = spark.read.csv(\"data/Human Development Index.csv\",inferSchema=True, header=True, nullValue=\"..\")\n",
    "\n",
    "#Explore missing values in the dataset World Development Indicators\n",
    "HDIStats.select([count(when(col(c).isNull(), c)).alias(c) for c in HDIStats.columns]).show()\n",
    "HDIStats.describe().filter(col(\"summary\")  == \"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+------------------------------+---+--------------+----------+\n",
      "|Country Name|Year|Employment to population ratio|GDP|GDP per capita|Gini index|\n",
      "+------------+----+------------------------------+---+--------------+----------+\n",
      "|           0|   0|                          2082|965|           968|      5394|\n",
      "+------------+----+------------------------------+---+--------------+----------+\n",
      "\n",
      "+-------+------------+----+------------------------------+----+--------------+----------+\n",
      "|summary|Country Name|Year|Employment to population ratio| GDP|GDP per capita|Gini index|\n",
      "+-------+------------+----+------------------------------+----+--------------+----------+\n",
      "|  count|        6944|6944|                          4862|5979|          5976|      1550|\n",
      "+-------+------------+----+------------------------------+----+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Explore missing values in the dataset World Development Indicators\n",
    "WDIStats.select([count(when(col(c).isNull(), c)).alias(c) for c in WDIStats.columns]).show()\n",
    "WDIStats.describe().filter(col(\"summary\")  == \"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'axes.color_cycle is not a valid rc parameter. See rcParams.keys() for a list of valid parameters.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, val)\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m                 \u001b[0mcval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mve\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'axes.color_cycle'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b18585df883c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/seaborn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mxkcd_rgb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxkcd_rgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcrayons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcrayons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.6.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/seaborn/rcmod.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(context, style, palette, font, font_scale, color_codes, rc)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mset_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mset_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"font.family\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfont\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mset_palette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_codes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor_codes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/seaborn/rcmod.py\u001b[0m in \u001b[0;36mset_palette\u001b[0;34m(palette, n_colors, desat, color_codes)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \"\"\"\n\u001b[1;32m    490\u001b[0m     \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpalettes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_palette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_colors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m     \u001b[0mmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"axes.color_cycle\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m     \u001b[0mmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"patch.facecolor\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_codes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, val)\u001b[0m\n\u001b[1;32m    927\u001b[0m             raise KeyError(\n\u001b[1;32m    928\u001b[0m                 \u001b[0;34m'%s is not a valid rc parameter. See rcParams.keys() for a '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 'list of valid parameters.' % (key,))\n\u001b[0m\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'axes.color_cycle is not a valid rc parameter. See rcParams.keys() for a list of valid parameters.'"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "#Explore the extreme values and outliers of the dataset WHO Suicide Statistics\n",
    "suicidesDf_pandas = SuicideStats.toPandas()\n",
    "\n",
    "#Explore the extreme values and outliers of the dataset Human Development Index\n",
    "#Explore the extreme values and outliers of the dataset World Development Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "#Explore the data pattern of the dataset WHO Suicide Statistics\n",
    "#Explore the data pattern of the dataset Human Development Index\n",
    "#Explore the data pattern of the dataset World Development Indicators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43776, 6)\n",
      "(38316, 6)\n"
     ]
    }
   ],
   "source": [
    "#Data Manipulation\n",
    "from pyspark.sql.types import (StructField,StringType,IntegerType,StructType)\n",
    "\n",
    "#Selecting Items\n",
    "#select records with populartion greater than 0\n",
    "print((SuicideStats.count(), len(SuicideStats.columns)))\n",
    "\n",
    "SuicideStats.createOrReplaceTempView('suicideStats')\n",
    "SuicideStats = spark.sql(\"SELECT * FROM suicideStats WHERE population >= 0\")\n",
    "print((SuicideStats.count(), len(SuicideStats.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189, 31)\n",
      "(189, 30)\n"
     ]
    }
   ],
   "source": [
    "#remove HDI Rank from HDIStats\n",
    "print((HDIStats.count(), len(HDIStats.columns)))\n",
    "\n",
    "HDIStats = HDIStats.drop(\"HDI Rank\")\n",
    "\n",
    "print((HDIStats.count(), len(HDIStats.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|          Country| 1990| 1991| 1992| 1993| 1994| 1995| 1996| 1997| 1998| 1999| 2000| 2001| 2002| 2003| 2004| 2005| 2006| 2007| 2008| 2009| 2010| 2011| 2012| 2013| 2014| 2015| 2016| 2017| 2018|\n",
      "+-----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|      Afghanistan|0.298|0.304|0.312|0.308|0.303|0.327|0.331|0.335|0.339|0.343|0.345|0.347|0.378|0.387|  0.4| 0.41|0.419|0.431|0.436|0.447|0.464|0.465|0.479|0.485|0.488| 0.49|0.491|0.493|0.496|\n",
      "|          Albania|0.644|0.625|0.608|0.611|0.617|0.629|0.639|0.639|0.649| 0.66|0.667|0.673| 0.68|0.687|0.692|0.702|0.709|0.718|0.724|0.729| 0.74|0.759|0.771|0.781|0.787|0.788|0.788|0.789|0.791|\n",
      "|          Algeria|0.578|0.582|0.589|0.593|0.597|0.602| 0.61|0.619|0.629|0.638|0.646|0.655|0.666|0.676|0.685|0.694|0.699|0.708|0.711| 0.72| 0.73|0.738|0.737|0.746|0.749|0.751|0.755|0.758|0.759|\n",
      "|          Andorra| null| null| null| null| null| null| null| null| null| null|0.759|0.767| 0.78| 0.82|0.826|0.819|0.829|0.829|0.831| 0.83|0.828|0.827|0.849|0.846|0.853| 0.85|0.854|0.852|0.857|\n",
      "|           Angola| null| null| null| null| null| null| null| null| null|0.384|0.394|0.404|0.419|0.428| 0.44|0.453|0.466|0.482|0.494|0.508| 0.51|0.525|0.537|0.547|0.557|0.565| 0.57|0.576|0.574|\n",
      "|AntiguaandBarbuda| null| null| null| null| null| null| null| null| null| null| null| null| null| null| null|0.773|0.779|0.784|0.783|0.776|0.772|0.764|0.767|0.767|0.767| 0.77|0.772|0.774|0.776|\n",
      "+-----------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "HDIStats.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38316, 6)\n",
      "(36060, 6)\n",
      "+-------+----+---+---+-----------+----------+\n",
      "|country|year|sex|age|suicides_no|population|\n",
      "+-------+----+---+---+-----------+----------+\n",
      "|      0|   0|  0|  0|          0|         0|\n",
      "+-------+----+---+---+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Data cleaning\n",
    "#remove misssing values from WHO Suicide Statistics\n",
    "print((SuicideStats.count(), len(SuicideStats.columns)))\n",
    "SuicideStats = SuicideStats.na.drop(subset=\"suicides_no\")\n",
    "print((SuicideStats.count(), len(SuicideStats.columns)))\n",
    "\n",
    "#Display the number of null value in each column after removing missing values\n",
    "SuicideStats.select([count(when(col(c).isNull(), c)).alias(c) for c in SuicideStats.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|          Country|                mean|\n",
      "+-----------------+--------------------+\n",
      "|      Afghanistan| -0.6149666666666667|\n",
      "|          Albania|-0.32346666666666657|\n",
      "|          Algeria|              -0.346|\n",
      "|          Andorra| -0.4764666666666667|\n",
      "|           Angola| -0.6722333333333333|\n",
      "|AntiguaandBarbuda|             -0.6392|\n",
      "+-----------------+--------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate mean value of each country\n",
    "\n",
    "from pyspark.sql.functions import mean\n",
    "\n",
    "HDIStatsMean = HDIStats.na.fill(0)\n",
    "HDIStatsMean = HDIStatsMean.select(col('Country'), (sum(col(x) for x in HDIStats.columns[1:]) / len(HDIStats.columns)-1).alias(\"mean\"))\n",
    "\n",
    "HDIStatsMean.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|Country|1990|1991|1992|1993|1994|1995|1996|1997|1998|1999|2000|2001|2002|2003|2004|2005|2006|2007|2008|2009|2010|2011|2012|2013|2014|2015|2016|2017|2018|\n",
      "+-------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|      0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|\n",
      "+-------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "HDIStats = HDIStats.alias('a')\\\n",
    "    .join(HDIStatsMean.alias('b'),col('b.Country') == col('a.Country'))\\\n",
    "    .select([when(col('a.'+xx).isNull(), col('b.mean')).otherwise(col('a.'+xx)).alias(xx) for xx in HDIStats.columns])\n",
    "\n",
    "#Display the number of null value in each column after removing missing values\n",
    "HDIStats.select([count(when(col(c).isNull(), c)).alias(c) for c in HDIStats.columns]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+------------------------------+---+--------------+----------+\n",
      "|Country Name|Year|Employment to population ratio|GDP|GDP per capita|Gini index|\n",
      "+------------+----+------------------------------+---+--------------+----------+\n",
      "|           0|   0|                          2082|965|           968|      5394|\n",
      "+------------+----+------------------------------+---+--------------+----------+\n",
      "\n",
      "+------------+----+------------------------------+---+--------------+----------+\n",
      "|Country Name|Year|Employment to population ratio|GDP|GDP per capita|Gini index|\n",
      "+------------+----+------------------------------+---+--------------+----------+\n",
      "|           0|   0|                             0|  0|             0|         0|\n",
      "+------------+----+------------------------------+---+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cleanig WDIStats\n",
    "#replace missing value with the mean value of this country\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "WDIStats.select([count(when(col(c).isNull(), c)).alias(c) for c in WDIStats.columns]).show()\n",
    "\n",
    "def fill_with_mean(df, exclude=set()): \n",
    "    stats = df.agg(*(\n",
    "        avg(c).alias(c) for c in df.columns if c not in exclude\n",
    "    ))\n",
    "    return df.na.fill(stats.first().asDict())\n",
    "\n",
    "WDIStats = fill_with_mean(WDIStats, [\"Country Name\", \"Year\"])\n",
    "\n",
    "WDIStats.select([count(when(col(c).isNull(), c)).alias(c) for c in WDIStats.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36060, 6)\n",
      "(28783, 6)\n"
     ]
    }
   ],
   "source": [
    "#remove extreme and outliers\n",
    "from pyspark.sql import DataFrameStatFunctions\n",
    "\n",
    "print((SuicideStats.count(), len(SuicideStats.columns)))\n",
    "\n",
    "bounds = {\n",
    "    c: dict(\n",
    "        zip([\"q1\", \"q3\"], SuicideStats.approxQuantile(c, [0.25, 0.75], 0))\n",
    "    )\n",
    "    for c in SuicideStats.columns if c in [\"suicides_no\", \"population\"]\n",
    "}\n",
    "\n",
    "for c in bounds:\n",
    "    iqr = bounds[c]['q3'] - bounds[c]['q1']\n",
    "    bounds[c]['lower'] = bounds[c]['q1'] - (iqr * 1.5)\n",
    "    bounds[c]['upper'] = bounds[c]['q3'] + (iqr * 1.5)\n",
    "\n",
    "SuicideStats = SuicideStats.select(\n",
    "    \"*\",\n",
    "    *[\n",
    "        when(\n",
    "            col(c).between(bounds[c]['lower'], bounds[c]['upper']),\n",
    "            0\n",
    "        ).otherwise(1).alias(c+\"_out\") \n",
    "        for c in SuicideStats.columns  if c in [\"suicides_no\", \"population\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "SuicideStats.createOrReplaceTempView('SuicideStats')\n",
    "SuicideStats = spark.sql(\"SELECT country, year, sex, age, suicides_no, population FROM SuicideStats WHERE suicides_no_out = 0 and population_out=0 \")\n",
    "print((SuicideStats.count(), len(SuicideStats.columns)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28783, 6)\n",
      "(28783, 7)\n",
      "+-------+----+------+-----------+-----------+----------+------------------+\n",
      "|country|year|   sex|        age|suicides_no|population|    SuicidePer100k|\n",
      "+-------+----+------+-----------+-----------+----------+------------------+\n",
      "|Albania|1987|female|15-24 years|         14|    289700|4.8325854332067655|\n",
      "|Albania|1987|female|25-34 years|          4|    257200|1.5552099533437014|\n",
      "|Albania|1987|female|35-54 years|          6|    278800|2.1520803443328553|\n",
      "|Albania|1987|female| 5-14 years|          0|    311000|               0.0|\n",
      "|Albania|1987|female|55-74 years|          0|    144600|               0.0|\n",
      "|Albania|1987|female|  75+ years|          1|     35600| 2.808988764044944|\n",
      "+-------+----+------+-----------+-----------+----------+------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Construct the Data\n",
    "#add new feature suicidePer100k to SuicideStats\n",
    "\n",
    "print((SuicideStats.count(), len(SuicideStats.columns)))\n",
    "SuicideStats = SuicideStats.withColumn(\"SuicidePer100k\", col(\"suicides_no\")/col(\"population\")*100000)\n",
    "\n",
    "print((SuicideStats.count(), len(SuicideStats.columns)))\n",
    "SuicideStats.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|    Country| 1990| 1991| 1992| 1993| 1994| 1995| 1996| 1997| 1998| 1999| 2000| 2001| 2002| 2003| 2004| 2005| 2006| 2007| 2008| 2009| 2010| 2011| 2012| 2013| 2014| 2015| 2016| 2017| 2018|\n",
      "+-----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|Afghanistan|0.298|0.304|0.312|0.308|0.303|0.327|0.331|0.335|0.339|0.343|0.345|0.347|0.378|0.387|  0.4| 0.41|0.419|0.431|0.436|0.447|0.464|0.465|0.479|0.485|0.488| 0.49|0.491|0.493|0.496|\n",
      "|    Albania|0.644|0.625|0.608|0.611|0.617|0.629|0.639|0.639|0.649| 0.66|0.667|0.673| 0.68|0.687|0.692|0.702|0.709|0.718|0.724|0.729| 0.74|0.759|0.771|0.781|0.787|0.788|0.788|0.789|0.791|\n",
      "|    Algeria|0.578|0.582|0.589|0.593|0.597|0.602| 0.61|0.619|0.629|0.638|0.646|0.655|0.666|0.676|0.685|0.694|0.699|0.708|0.711| 0.72| 0.73|0.738|0.737|0.746|0.749|0.751|0.755|0.758|0.759|\n",
      "+-----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------+----+--------+\n",
      "|    Country|Year|HDIScore|\n",
      "+-----------+----+--------+\n",
      "|Afghanistan|1990|   0.298|\n",
      "|Afghanistan|1991|   0.304|\n",
      "|Afghanistan|1992|   0.312|\n",
      "|Afghanistan|1993|   0.308|\n",
      "|Afghanistan|1994|   0.303|\n",
      "|Afghanistan|1995|   0.327|\n",
      "+-----------+----+--------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#transport HDI score \n",
    "#HDIStats.columns\n",
    "from pyspark.sql.functions import array, col, explode, struct, lit\n",
    "\n",
    "HDIStats.show(3)\n",
    "\n",
    "cols, dtypes = zip(*((c, t) for (c, t) in HDIStats.dtypes if c not in ['Country']))\n",
    "kvs = explode(array([\n",
    "      struct(lit(c).alias(\"Year\"), col(c).alias(\"HDIScore\")) for c in cols\n",
    "    ])).alias(\"kvs\")\n",
    "    \n",
    "HDIStats = HDIStats.select(['Country'] + [kvs]).select(['Country'] + [\"kvs.Year\", \"kvs.HDIScore\"])\n",
    "HDIStats.show(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17665, 12)\n",
      "+-------+----+----+-----------+----------+----------+------------------+--------+---------------------------+-------------+------------+-----------------+\n",
      "|Country|Year| Sex|        Age|SuicidesNo|Population|    SuicidePer100k|HDIScore|EmploymentToPopulationRatio|          GDP|GDPPerCapita|        GiniIndex|\n",
      "+-------+----+----+-----------+----------+----------+------------------+--------+---------------------------+-------------+------------+-----------------+\n",
      "|Albania|1992|male|  75+ years|         0|     23900|               0.0|   0.608|                54.30599976|4.038036613E9| 1243.605824|38.62148387096775|\n",
      "|Albania|1992|male|55-74 years|         5|    159500| 3.134796238244514|   0.608|                54.30599976|4.038036613E9| 1243.605824|38.62148387096775|\n",
      "|Albania|1992|male| 5-14 years|         0|    362900|               0.0|   0.608|                54.30599976|4.038036613E9| 1243.605824|38.62148387096775|\n",
      "|Albania|1992|male|35-54 years|        12|    343800|3.4904013961605584|   0.608|                54.30599976|4.038036613E9| 1243.605824|38.62148387096775|\n",
      "|Albania|1992|male|25-34 years|         7|    245500|2.8513238289205702|   0.608|                54.30599976|4.038036613E9| 1243.605824|38.62148387096775|\n",
      "|Albania|1992|male|15-24 years|         9|    263700|3.4129692832764507|   0.608|                54.30599976|4.038036613E9| 1243.605824|38.62148387096775|\n",
      "+-------+----+----+-----------+----------+----------+------------------+--------+---------------------------+-------------+------------+-----------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Integrate Data Sources\n",
    "SuicideStats = SuicideStats.withColumnRenamed(\"country\",\"Country\")\\\n",
    "    .withColumnRenamed(\"year\",\"Year\")\\\n",
    "    .withColumnRenamed(\"sex\",\"Sex\")\\\n",
    "    .withColumnRenamed(\"age\",\"Age\")\\\n",
    "    .withColumnRenamed(\"suicides_no\",\"SuicidesNo\")\\\n",
    "    .withColumnRenamed(\"population\",\"Population\")\n",
    "\n",
    "WDIStats = WDIStats.withColumnRenamed(\"Country Name\",\"Country\")\\\n",
    "    .withColumnRenamed(\"Employment to population ratio\",\"EmploymentToPopulationRatio\")\\\n",
    "    .withColumnRenamed(\"GDP per capita\",\"GDPPerCapita\")\\\n",
    "    .withColumnRenamed(\"Gini index\",\"GiniIndex\")\n",
    "\n",
    "datasource = SuicideStats\\\n",
    "    .join(HDIStats, (SuicideStats.Country == HDIStats.Country) & (SuicideStats.Year == HDIStats.Year))\\\n",
    "    .join(WDIStats, (SuicideStats.Country == WDIStats.Country) & (SuicideStats.Year == WDIStats.Year))\\\n",
    "    .select([SuicideStats.Country, SuicideStats.Year, SuicideStats.Sex, SuicideStats.Age, SuicideStats.SuicidesNo,\n",
    "             SuicideStats.Population, SuicideStats.SuicidePer100k, HDIStats.HDIScore,\n",
    "             WDIStats.EmploymentToPopulationRatio, WDIStats.GDP, WDIStats.GDPPerCapita, WDIStats.GiniIndex])\n",
    "\n",
    "print((datasource.count(), len(datasource.columns)))\n",
    "datasource.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17665, 12)\n",
      "+-------+----+----+-----------+----------+----------+------------------+--------+---------------------------+-------------+------------+-----------------+\n",
      "|Country|Year| Sex|        Age|SuicidesNo|Population|    SuicidePer100k|HDIScore|EmploymentToPopulationRatio|          GDP|GDPPerCapita|        GiniIndex|\n",
      "+-------+----+----+-----------+----------+----------+------------------+--------+---------------------------+-------------+------------+-----------------+\n",
      "|Albania|1992|male|  75+ years|         0|     23900|               0.0|   0.608|                54.30599976|4.038036613E9| 1243.605824|38.62148387096775|\n",
      "|Albania|1992|male|55-74 years|         5|    159500| 3.134796238244514|   0.608|                54.30599976|4.038036613E9| 1243.605824|38.62148387096775|\n",
      "|Albania|1992|male| 5-14 years|         0|    362900|               0.0|   0.608|                54.30599976|4.038036613E9| 1243.605824|38.62148387096775|\n",
      "|Albania|1992|male|35-54 years|        12|    343800|3.4904013961605584|   0.608|                54.30599976|4.038036613E9| 1243.605824|38.62148387096775|\n",
      "|Albania|1992|male|25-34 years|         7|    245500|2.8513238289205702|   0.608|                54.30599976|4.038036613E9| 1243.605824|38.62148387096775|\n",
      "|Albania|1992|male|15-24 years|         9|    263700|3.4129692832764507|   0.608|                54.30599976|4.038036613E9| 1243.605824|38.62148387096775|\n",
      "+-------+----+----+-----------+----------+----------+------------------+--------+---------------------------+-------------+------------+-----------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasource = SuicideStats\\\n",
    "    .join(HDIStats, (SuicideStats.Country == HDIStats.Country) & (SuicideStats.Year == HDIStats.Year))\\\n",
    "    .join(WDIStats, (SuicideStats.Country == WDIStats.Country) & (SuicideStats.Year == WDIStats.Year))\\\n",
    "    .select([SuicideStats.Country, SuicideStats.Year, SuicideStats.Sex, SuicideStats.Age, SuicideStats.SuicidesNo,\n",
    "             SuicideStats.Population, SuicideStats.SuicidePer100k, HDIStats.HDIScore,\n",
    "             WDIStats.EmploymentToPopulationRatio, WDIStats.GDP, WDIStats.GDPPerCapita, WDIStats.GiniIndex])\n",
    "\n",
    "print((datasource.count(), len(datasource.columns)))\n",
    "datasource.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- SuicidesNo: integer (nullable = true)\n",
      " |-- Population: integer (nullable = true)\n",
      " |-- SuicidePer100k: double (nullable = true)\n",
      " |-- HDIScore: double (nullable = true)\n",
      " |-- EmploymentToPopulationRatio: double (nullable = false)\n",
      " |-- GDP: double (nullable = false)\n",
      " |-- GDPPerCapita: double (nullable = false)\n",
      " |-- GiniIndex: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasource.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce the data\n",
    "#todo explore feature importance\n",
    "#drop some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+-----------+----------+----------+------------------+--------+---------------------------+-------------+------------+-----------------+\n",
      "|Country|Year| Sex|        Age|SuicidesNo|Population|    SuicidePer100k|HDIScore|EmploymentToPopulationRatio|          GDP|GDPPerCapita|        GiniIndex|\n",
      "+-------+----+----+-----------+----------+----------+------------------+--------+---------------------------+-------------+------------+-----------------+\n",
      "|Albania|1992|male|  75+ years|         0|     23900|               0.0|   0.608|                54.30599976|4.038036613E9| 1243.605824|38.62148387096775|\n",
      "|Albania|1992|male|55-74 years|         5|    159500| 3.134796238244514|   0.608|                54.30599976|4.038036613E9| 1243.605824|38.62148387096775|\n",
      "|Albania|1992|male| 5-14 years|         0|    362900|               0.0|   0.608|                54.30599976|4.038036613E9| 1243.605824|38.62148387096775|\n",
      "|Albania|1992|male|35-54 years|        12|    343800|3.4904013961605584|   0.608|                54.30599976|4.038036613E9| 1243.605824|38.62148387096775|\n",
      "|Albania|1992|male|25-34 years|         7|    245500|2.8513238289205702|   0.608|                54.30599976|4.038036613E9| 1243.605824|38.62148387096775|\n",
      "|Albania|1992|male|15-24 years|         9|    263700|3.4129692832764507|   0.608|                54.30599976|4.038036613E9| 1243.605824|38.62148387096775|\n",
      "+-------+----+----+-----------+----------+----------+------------------+--------+---------------------------+-------------+------------+-----------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Project the data\n",
    "#log, nomalise\n",
    "datasource.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 12335\n",
      "Test Dataset Count: 5330\n"
     ]
    }
   ],
   "source": [
    "#Train and Test Split\n",
    "# Split the data\n",
    "(train, test) = tmp.randomSplit([0.7, 0.3], seed =722)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# casts all columns to a numeric type\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "sexIndexer = StringIndexer(inputCol=\"Sex\", outputCol=\"SexIndex\")\n",
    "ageIndexer = StringIndexer(inputCol=\"Age\", outputCol=\"AgeIndex\")\n",
    "\n",
    "featuresCols = ['HDIScore', 'EmploymentToPopulationRatio', 'GDP', 'GDPPerCapita', 'GiniIndex', 'SexIndex', 'AgeIndex']\n",
    "featureAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"rawFeatures\")\n",
    "featureIndexer = VectorIndexer(inputCol=\"rawFeatures\", outputCol=\"indexedFeatures\", maxCategories=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# Takes the \"features\" column and learns to predict \"SuicidePer100k\"\n",
    "gbt = GBTRegressor(featuresCol=\"indexedFeatures\",labelCol=\"SuicidePer100k\", maxIter=5, maxDepth=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Define a grid of hyperparameters to test:\n",
    "#  - maxDepth: max depth of each decision tree in the GBT ensemble\n",
    "#  - maxIter: iterations, i.e., number of trees in each GBT ensemble\n",
    "# In this example notebook, we keep these values small.  In practice, to get the highest accuracy, you would likely want to try deeper trees (10 or higher) and more trees in the ensemble (>100)\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "  .addGrid(gbt.maxDepth, [2, 5])\\\n",
    "  .addGrid(gbt.maxIter, [2, 5])\\\n",
    "  .build()\n",
    "\n",
    "# We define an evaluation metric.  This tells CrossValidator how well we are doing by comparing the true labels with predictions.\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=gbt.getLabelCol(), predictionCol=gbt.getPredictionCol())\n",
    "\n",
    "# Declare the CrossValidator, which runs model tuning for us.\n",
    "cv = CrossValidator(estimator=gbt, evaluator=evaluator, estimatorParamMaps=paramGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[sexIndexer, ageIndexer, featureAssembler, featureIndexer, gbt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-49-5e0d19b81d99>\", line 1, in <module>\n",
      "    pipelineModel = pipeline.fit(train)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/pipeline.py\", line 108, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 236, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 232, in _fit_java\n",
      "    self._transfer_params_to_java()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 95, in _transfer_params_to_java\n",
      "    pair = self._make_java_param_pair(param, paramMap[param])\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 84, in _make_java_param_pair\n",
      "    java_param = self._java_obj.getParam(param.name)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1131, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-49-5e0d19b81d99>\", line 1, in <module>\n",
      "    pipelineModel = pipeline.fit(train)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/pipeline.py\", line 108, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 236, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 232, in _fit_java\n",
      "    self._transfer_params_to_java()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 95, in _transfer_params_to_java\n",
      "    pair = self._make_java_param_pair(param, paramMap[param])\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 84, in _make_java_param_pair\n",
      "    java_param = self._java_obj.getParam(param.name)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1131, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-49-5e0d19b81d99>\", line 1, in <module>\n",
      "    pipelineModel = pipeline.fit(train)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/pipeline.py\", line 108, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 236, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 232, in _fit_java\n",
      "    self._transfer_params_to_java()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 95, in _transfer_params_to_java\n",
      "    pair = self._make_java_param_pair(param, paramMap[param])\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 84, in _make_java_param_pair\n",
      "    java_param = self._java_obj.getParam(param.name)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1131, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-49-5e0d19b81d99>\", line 1, in <module>\n",
      "    pipelineModel = pipeline.fit(train)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/pipeline.py\", line 108, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 236, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 232, in _fit_java\n",
      "    self._transfer_params_to_java()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 95, in _transfer_params_to_java\n",
      "    pair = self._make_java_param_pair(param, paramMap[param])\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 84, in _make_java_param_pair\n",
      "    java_param = self._java_obj.getParam(param.name)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1131, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-49-5e0d19b81d99>\", line 1, in <module>\n",
      "    pipelineModel = pipeline.fit(train)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/pipeline.py\", line 108, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 236, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 232, in _fit_java\n",
      "    self._transfer_params_to_java()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 95, in _transfer_params_to_java\n",
      "    pair = self._make_java_param_pair(param, paramMap[param])\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 84, in _make_java_param_pair\n",
      "    java_param = self._java_obj.getParam(param.name)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1131, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-49-5e0d19b81d99>\", line 1, in <module>\n",
      "    pipelineModel = pipeline.fit(train)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/pipeline.py\", line 108, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 236, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 232, in _fit_java\n",
      "    self._transfer_params_to_java()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 95, in _transfer_params_to_java\n",
      "    pair = self._make_java_param_pair(param, paramMap[param])\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 84, in _make_java_param_pair\n",
      "    java_param = self._java_obj.getParam(param.name)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1131, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-49-5e0d19b81d99>\", line 1, in <module>\n",
      "    pipelineModel = pipeline.fit(train)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/pipeline.py\", line 108, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 236, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 232, in _fit_java\n",
      "    self._transfer_params_to_java()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 95, in _transfer_params_to_java\n",
      "    pair = self._make_java_param_pair(param, paramMap[param])\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 84, in _make_java_param_pair\n",
      "    java_param = self._java_obj.getParam(param.name)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1131, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-49-5e0d19b81d99>\", line 1, in <module>\n",
      "    pipelineModel = pipeline.fit(train)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/pipeline.py\", line 108, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 236, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 232, in _fit_java\n",
      "    self._transfer_params_to_java()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 95, in _transfer_params_to_java\n",
      "    pair = self._make_java_param_pair(param, paramMap[param])\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 84, in _make_java_param_pair\n",
      "    java_param = self._java_obj.getParam(param.name)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1131, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-49-5e0d19b81d99>\", line 1, in <module>\n",
      "    pipelineModel = pipeline.fit(train)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/pipeline.py\", line 108, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 236, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 232, in _fit_java\n",
      "    self._transfer_params_to_java()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 95, in _transfer_params_to_java\n",
      "    pair = self._make_java_param_pair(param, paramMap[param])\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 84, in _make_java_param_pair\n",
      "    java_param = self._java_obj.getParam(param.name)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1131, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-49-5e0d19b81d99>\", line 1, in <module>\n",
      "    pipelineModel = pipeline.fit(train)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/pipeline.py\", line 108, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 236, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 232, in _fit_java\n",
      "    self._transfer_params_to_java()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 95, in _transfer_params_to_java\n",
      "    pair = self._make_java_param_pair(param, paramMap[param])\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 84, in _make_java_param_pair\n",
      "    java_param = self._java_obj.getParam(param.name)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1131, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-49-5e0d19b81d99>\", line 1, in <module>\n",
      "    pipelineModel = pipeline.fit(train)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/pipeline.py\", line 108, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 236, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 232, in _fit_java\n",
      "    self._transfer_params_to_java()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 95, in _transfer_params_to_java\n",
      "    pair = self._make_java_param_pair(param, paramMap[param])\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 84, in _make_java_param_pair\n",
      "    java_param = self._java_obj.getParam(param.name)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1131, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-49-5e0d19b81d99>\", line 1, in <module>\n",
      "    pipelineModel = pipeline.fit(train)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/pipeline.py\", line 108, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 236, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 232, in _fit_java\n",
      "    self._transfer_params_to_java()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 95, in _transfer_params_to_java\n",
      "    pair = self._make_java_param_pair(param, paramMap[param])\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 84, in _make_java_param_pair\n",
      "    java_param = self._java_obj.getParam(param.name)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1131, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-49-5e0d19b81d99>\", line 1, in <module>\n",
      "    pipelineModel = pipeline.fit(train)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/pipeline.py\", line 108, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 236, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 232, in _fit_java\n",
      "    self._transfer_params_to_java()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 95, in _transfer_params_to_java\n",
      "    pair = self._make_java_param_pair(param, paramMap[param])\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 84, in _make_java_param_pair\n",
      "    java_param = self._java_obj.getParam(param.name)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1131, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-49-5e0d19b81d99>\", line 1, in <module>\n",
      "    pipelineModel = pipeline.fit(train)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/pipeline.py\", line 108, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 236, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 232, in _fit_java\n",
      "    self._transfer_params_to_java()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 95, in _transfer_params_to_java\n",
      "    pair = self._make_java_param_pair(param, paramMap[param])\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 84, in _make_java_param_pair\n",
      "    java_param = self._java_obj.getParam(param.name)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1131, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:38398)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:38398)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-5e0d19b81d99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipelineModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0mJava\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \"\"\"\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transfer_params_to_java\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparamMap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mpair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_java_param_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_make_java_param_pair\u001b[0;34m(self, param, value)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolveParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mjava_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mjava_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    879\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \"\"\"\n\u001b[0;32m--> 881\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    833\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    834\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    968\u001b[0m                 \u001b[0;34m\"server ({0}:{1})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:38398)"
     ]
    }
   ],
   "source": [
    "pipelineModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipelineModel.transform(test)\n",
    "display(predictions.select(\"SuicidePer100k\", \"prediction\", *featuresCols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"SuicidePer100k\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
